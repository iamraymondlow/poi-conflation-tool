{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy.fuzz import token_set_ratio\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, accuracy_score, f1_score\n",
    "from joblib import dump\n",
    "\n",
    "# load config file\n",
    "with open('../config.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    This class perform model training for identifying duplicated POIs between different data sources.\n",
    "    \"\"\"\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        This function performs model training for identifying duplicated POIs between different data\n",
    "        sources.\n",
    "        \"\"\"\n",
    "        # load manually labeled data\n",
    "        print('Loading manually labeled data for model training...')\n",
    "        manual_data = pd.read_csv(\"../data/labeled_data/cbp_manual.csv\")\n",
    "        manual_data['duplicates'] = manual_data['duplicates'].apply(self._format_duplicates)\n",
    "        manual_data = manual_data[['properties.address.formatted_address', 'properties.name',\n",
    "                                   'lat', 'lng', 'id', 'duplicates']]\n",
    "        manual_data = gpd.GeoDataFrame(manual_data,\n",
    "                                       geometry=gpd.points_from_xy(manual_data['lng'],\n",
    "                                                                   manual_data['lat']))\n",
    "\n",
    "        # process manually labeled data\n",
    "        print('Processing manually labeled data for model training...')\n",
    "        train_test_data = self._process_manual_data(manual_data)\n",
    "        train_test_data = pd.DataFrame(train_test_data, columns=['address_str_similarity', 'name_str_similarity', \n",
    "                                                                 'address_similarity', 'name_similarity', \n",
    "                                                                 'label'])\n",
    "\n",
    "        # perform data sampling to balance class distribution\n",
    "        print('Performing data sampling to balance class distribution...')\n",
    "        # comment this out if you do not intend to perform data rebalancing  #TODO\n",
    "#         train_datasets, test_data = self._perform_data_rebalancing(train_test_data)\n",
    "        # comment this out if you intend to perform data rebalancing\n",
    "        train_datasets, test_data = self._no_data_rebalancing(train_test_data)\n",
    "        \n",
    "        # train models\n",
    "        print('Begin model training...')\n",
    "        gb_models = self._train(train_datasets, 'GB')\n",
    "        rf_models = self._train(train_datasets, 'RF')\n",
    "        svm_models = self._train(train_datasets, 'SVM')\n",
    "\n",
    "        # evaluate model performance on hold out set\n",
    "        print('Perform model evaluation...')\n",
    "        # string  #TODO\n",
    "#         y_pred_gb = self._predict(gb_models, test_data[['address_str_similarity', 'name_str_similarity']])\n",
    "#         y_pred_rf = self._predict(rf_models, test_data[['address_str_similarity', 'name_str_similarity']])\n",
    "#         y_pred_svm = self._predict(svm_models, test_data[['address_str_similarity', 'name_str_similarity']])\n",
    "        # TFIDF\n",
    "#         y_pred_gb = self._predict(gb_models, test_data[['address_similarity', 'name_similarity']])\n",
    "#         y_pred_rf = self._predict(rf_models, test_data[['address_similarity', 'name_similarity']])\n",
    "#         y_pred_svm = self._predict(svm_models, test_data[['address_similarity', 'name_similarity']])\n",
    "        # string + TFIDF\n",
    "        y_pred_gb = self._predict(gb_models, test_data[['address_similarity', 'name_str_similarity']])\n",
    "        y_pred_rf = self._predict(rf_models, test_data[['address_similarity', 'name_str_similarity']])\n",
    "        y_pred_svm = self._predict(svm_models, test_data[['address_similarity', 'name_str_similarity']])\n",
    "        \n",
    "        self._evaluate(test_data['label'], y_pred_gb, 'Gradient Boosting')\n",
    "        self._evaluate(test_data['label'], y_pred_rf, 'Random Forest')\n",
    "        self._evaluate(test_data['label'], y_pred_svm, 'SVM')\n",
    "\n",
    "    def _evaluate(self, y_true, y_pred, algorithm):\n",
    "        \"\"\"\n",
    "        Evaluates the model performance based on overall accuracy, balanced accuracy and macro-ave\n",
    "        f1 score.\n",
    "\n",
    "        :param y_true: Series\n",
    "            Contains the ground truth labels for POI duplicates.\n",
    "        :param y_pred: np.array\n",
    "            Contains the model's inferred labels.\n",
    "        :param algorithm: str\n",
    "            Contains information about the algorithm under evaluation.\n",
    "        \"\"\"\n",
    "        print(algorithm)\n",
    "        print('Overall Accuracy: {}'.format(accuracy_score(y_true, y_pred)))\n",
    "        print('Balanced Accuracy: {}'.format(balanced_accuracy_score(y_true, y_pred)))\n",
    "        print('Macro-average F1 Score: {}'.format(f1_score(y_true, y_pred, average='macro')))\n",
    "        print(classification_report(y_true, y_pred, target_names=['Not Match', 'Match']))\n",
    "\n",
    "    def _perform_data_rebalancing(self, data):\n",
    "        \"\"\"\n",
    "        Performs bootstrapping and oversampling to rebalance the positive and negative class\n",
    "        before creating subsets of the original dataset.\n",
    "\n",
    "        :param data: Dataframe\n",
    "            Contains the original dataset with imbalanced\n",
    "\n",
    "        :return:\n",
    "        train_datasets: list of Dataframe\n",
    "            Contains subsets of the original dataset after bootstrapping and oversampling to\n",
    "            rebalance the positive and negative classes.\n",
    "        test_data: Dataframe\n",
    "            Contains a randomly sampled hold out set of the original dataset for model evaluation.\n",
    "        \"\"\"\n",
    "        # perform train test split\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "        train_data = data.iloc[int(len(data) * 0.25):]\n",
    "        test_data = data.iloc[:int(len(data) * 0.25)]\n",
    "\n",
    "        # extract positive and negative classes\n",
    "        positive_train = train_data[train_data['label'] == 1].sample(frac=1).reset_index(drop=True)\n",
    "        negative_train = train_data[train_data['label'] == 0].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # perform bootstrapping for the negative class and oversample the positive class\n",
    "        train_datasets = []\n",
    "        for i in range(4):\n",
    "            negative_sample = negative_train.sample(frac=0.75)\n",
    "            positive_sample = positive_train.sample(n=len(negative_sample), replace=True)\n",
    "            assert len(negative_sample) == len(positive_sample)\n",
    "            bootstrap_sample = pd.concat([negative_sample, positive_sample])\n",
    "            train_datasets.append(bootstrap_sample.sample(frac=1).reset_index(drop=True))\n",
    "\n",
    "        return train_datasets, test_data\n",
    "    \n",
    "    def _no_data_rebalancing(self, data):\n",
    "        \"\"\"\n",
    "        Do not perform bootstrapping and oversampling to rebalance the positive and negative class\n",
    "        before creating subsets of the original dataset.\n",
    "\n",
    "        :param data: Dataframe\n",
    "            Contains the original dataset with imbalanced\n",
    "\n",
    "        :return:\n",
    "        train_datasets: list of Dataframe\n",
    "            Contains subsets of the original dataset after bootstrapping and oversampling to\n",
    "            rebalance the positive and negative classes.\n",
    "        test_data: Dataframe\n",
    "            Contains a randomly sampled hold out set of the original dataset for model evaluation.\n",
    "        \"\"\"\n",
    "        # perform train test split\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "        train_data = data.iloc[int(len(data) * 0.25):].reset_index(drop=True)\n",
    "        test_data = data.iloc[:int(len(data) * 0.25)].reset_index(drop=True)\n",
    "        \n",
    "        return [train_data], test_data\n",
    "\n",
    "    def _format_duplicates(self, duplicate_string):\n",
    "        \"\"\"\n",
    "        Extracts the IDs of the duplicated POIs in a list format.\n",
    "\n",
    "        :param duplicate_string: str\n",
    "            Contains the duplicated IDs in string format.\n",
    "\n",
    "        :return:\n",
    "        duplicates: list\n",
    "            Contains the duplicated IDs in list format.\n",
    "        \"\"\"\n",
    "        duplicates = re.sub('[\\[\\]\\']', '', duplicate_string).split(', ')\n",
    "        if len(duplicates) == 1 and duplicates[0] == '':\n",
    "            return []\n",
    "        else:\n",
    "            return duplicates\n",
    "\n",
    "    def _buffer_in_meters(self, lng, lat, radius):\n",
    "        \"\"\"\n",
    "        Converts a latitude, longitude coordinate pair into a buffer with user-defined radius.s\n",
    "\n",
    "        :param lng: float\n",
    "            Contains the longitude information.\n",
    "        :param lat: float\n",
    "            Contains the latitude information.\n",
    "        :param radius: float\n",
    "            Contains the buffer radius in metres.\n",
    "\n",
    "        :return:\n",
    "        buffer_latlng: Polygon\n",
    "            Contains the buffer.\n",
    "        \"\"\"\n",
    "        proj_meters = pyproj.CRS('EPSG:3414')  # EPSG for Singapore\n",
    "        proj_latlng = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "        project_to_metres = pyproj.Transformer.from_crs(proj_latlng, proj_meters, always_xy=True).transform\n",
    "        project_to_latlng = pyproj.Transformer.from_crs(proj_meters, proj_latlng, always_xy=True).transform\n",
    "        pt_meters = transform(project_to_metres, Point(lng, lat))\n",
    "        buffer_meters = pt_meters.buffer(radius)\n",
    "        buffer_latlng = transform(project_to_latlng, buffer_meters)\n",
    "        return buffer_latlng\n",
    "\n",
    "    def _label_data(self, manual_data, centroid_idx, address_matrix, name_matrix):\n",
    "        \"\"\"\n",
    "        Generates the labeled data for the neighbouring POIs around a centroid POI.\n",
    "\n",
    "        :param manual_data: GeoDataFrame\n",
    "            Contains the manually labeled data and the ID information of their duplicates.\n",
    "        :param centroid_idx: int\n",
    "            Contains the index of the centroid POI.\n",
    "        :param address_matrix: np.array\n",
    "            Contains the address matrix after vectorising the address corpus using TFIDF.\n",
    "\n",
    "        :return:\n",
    "        np.array\n",
    "            Contains the labeled data containing the name and address similarity scores.\n",
    "        \"\"\"\n",
    "        # identify neighbouring POIs\n",
    "        buffer = self._buffer_in_meters(manual_data.loc[centroid_idx, 'lng'],\n",
    "                                        manual_data.loc[centroid_idx, 'lat'],\n",
    "                                        config['search_radius'])\n",
    "        neighbour_pois = manual_data[manual_data.intersects(buffer)]\n",
    "        neighbour_idx = list(neighbour_pois.index)\n",
    "\n",
    "        # calculate address similarity score for neighbouring POIs based on TFIDF\n",
    "        centroid_address = address_matrix[centroid_idx, :]\n",
    "        address_similarity = cosine_similarity(address_matrix[neighbour_idx, :], centroid_address).reshape(-1, 1)\n",
    "\n",
    "        # calculate address similarity score for neighbouring POIs based on string comparison\n",
    "        if pd.isnull(manual_data.loc[centroid_idx, 'properties.address.formatted_address']):\n",
    "            return None\n",
    "        address_str_similarity = np.array(\n",
    "            [token_set_ratio(manual_data.loc[centroid_idx, 'properties.address.formatted_address'].lower(),\n",
    "                             neighbour_address.lower())\n",
    "             if not pd.isnull(neighbour_address) else 0.0\n",
    "             for neighbour_address\n",
    "             in manual_data.loc[neighbour_idx, 'properties.address.formatted_address'].tolist()]\n",
    "        ).reshape(-1, 1)\n",
    "        \n",
    "        # calculate name similarity score for neighbouring POIs based on TFIDF\n",
    "        centroid_name = name_matrix[centroid_idx, :]\n",
    "        name_similarity = cosine_similarity(name_matrix[neighbour_idx, :], centroid_name).reshape(-1, 1)\n",
    "\n",
    "        # calculate name similarity score for neighbouring POIs based on string comparison\n",
    "        if pd.isnull(manual_data.loc[centroid_idx, 'properties.name']):\n",
    "            return None\n",
    "        name_str_similarity = np.array(\n",
    "            [token_set_ratio(manual_data.loc[centroid_idx, 'properties.name'].lower(),\n",
    "                             neighbour_name.lower())\n",
    "             if not pd.isnull(neighbour_name) else 0.0\n",
    "             for neighbour_name\n",
    "             in manual_data.loc[neighbour_idx, 'properties.name'].tolist()]\n",
    "        ).reshape(-1, 1)\n",
    "\n",
    "        # extract labels for neighbouring POIs\n",
    "        labels = np.zeros((len(neighbour_idx), 1))\n",
    "        for i in range(len(neighbour_idx)):\n",
    "            if manual_data.loc[neighbour_idx[i], 'id'] in manual_data.loc[centroid_idx, 'duplicates']:\n",
    "                labels[i, 0] = 1\n",
    "            elif manual_data.loc[neighbour_idx[i], 'id'] == manual_data.loc[centroid_idx, 'id']:\n",
    "                labels[i, 0] = 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return np.hstack((address_str_similarity, name_str_similarity, \n",
    "                          address_similarity, name_similarity, labels))\n",
    "\n",
    "    def _process_manual_data(self, manual_data):\n",
    "        \"\"\"\n",
    "        Processes the manually labeled data for model training and evaluation by identifying neighbouring POIs\n",
    "        and labeling them as either duplicates or not duplicates.\n",
    "\n",
    "        :param manual_data: GeoDataFrame\n",
    "            Contains the manually labeled data and the ID information of their duplicates.\n",
    "\n",
    "        :return:\n",
    "        labeled_data: np.array\n",
    "            Contains the labeled data ready for model training and evaluation.\n",
    "        \"\"\"\n",
    "        address_corpus = manual_data['properties.address.formatted_address'].fillna('Singapore').tolist()\n",
    "        address_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))\n",
    "        address_matrix = address_vectorizer.fit_transform(address_corpus)\n",
    "        \n",
    "        name_corpus = manual_data['properties.name'].fillna('Singapore').tolist()\n",
    "        name_vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 3))\n",
    "        name_matrix = name_vectorizer.fit_transform(name_corpus)\n",
    "\n",
    "        labeled_data = None\n",
    "        for i in tqdm(range(len(manual_data))):\n",
    "            temp_data = self._label_data(manual_data, i, address_matrix, name_matrix)\n",
    "\n",
    "            if (temp_data is not None) and (labeled_data is not None):\n",
    "                labeled_data = np.vstack((labeled_data, temp_data))\n",
    "            elif temp_data is not None:\n",
    "                labeled_data = temp_data\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return labeled_data\n",
    "\n",
    "    def _hyperparameter_tuning(self, train_data, algorithm):\n",
    "        \"\"\"\n",
    "        Performs hyperparmeter tuning based on the training dataset.\n",
    "\n",
    "        :param train_data: Dataframe\n",
    "            Contains the training data.\n",
    "        :param algorithm: str\n",
    "            Indicates the algorithm used for model training.\n",
    "\n",
    "        :return:\n",
    "        grid_search: sklearn.model object\n",
    "            Contains the trained model after hyperparameter tuning.\n",
    "        \"\"\"\n",
    "        if algorithm == 'GB':\n",
    "            parameters = {'n_estimators': [100],\n",
    "                          'min_samples_split': [2],\n",
    "                          'min_samples_leaf': [1],\n",
    "                          'max_depth': [3]}\n",
    "            model = GradientBoostingClassifier()\n",
    "\n",
    "        elif algorithm == 'RF':\n",
    "            parameters = {'n_estimators': [100],\n",
    "                          'min_samples_split': [2],\n",
    "                          'min_samples_leaf': [1]}\n",
    "            model = RandomForestClassifier()\n",
    "\n",
    "        elif algorithm == 'SVM':\n",
    "            parameters = {'C': [1.0],\n",
    "                          'kernel': ['rbf'],\n",
    "                          'degree': [3]}\n",
    "            model = SVC(probability=True)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('{} is not supported.'.format(algorithm))\n",
    "            \n",
    "            \n",
    "        #TODO\n",
    "#         grid_search = GridSearchCV(model, parameters, scoring=['balanced_accuracy', 'f1_macro'],\n",
    "#                                    n_jobs=-1, refit='balanced_accuracy')    \n",
    "        grid_search = GridSearchCV(model, parameters, scoring=['accuracy'], n_jobs=-1, refit='accuracy')\n",
    "        # string  #TODO\n",
    "#         grid_search.fit(train_data[['address_str_similarity', 'name_str_similarity']], train_data['label'])\n",
    "        # TFIDF\n",
    "#         grid_search.fit(train_data[['address_similarity', 'name_similarity']], train_data['label'])\n",
    "        # string + TDIDF\n",
    "        grid_search.fit(train_data[['address_similarity', 'name_str_similarity']], train_data['label'])\n",
    "\n",
    "        return grid_search\n",
    "\n",
    "    def _train(self, train_datasets, algorithm):\n",
    "        \"\"\"\n",
    "        Performs model training based on different subsets of the training data.\n",
    "        :param train_datasets: list of Dataframes\n",
    "            Contains a list of training data after rebalancing the number of positive and\n",
    "            negative classes.\n",
    "        :param algorithm: str\n",
    "            Indicates the algorithm used for model training.\n",
    "\n",
    "        :return:\n",
    "        models: list of sklearn.models\n",
    "            Contains the trained models.\n",
    "        \"\"\"\n",
    "        models = []\n",
    "        i = 1\n",
    "        for train_data in train_datasets:\n",
    "            print('Training {} model {}/{}...'.format(algorithm, i, len(train_datasets)))\n",
    "            i += 1\n",
    "            if algorithm == 'GB':\n",
    "                models.append(self._hyperparameter_tuning(train_data, 'GB'))\n",
    "            elif algorithm == 'RF':\n",
    "                models.append(self._hyperparameter_tuning(train_data, 'RF'))\n",
    "            elif algorithm == 'SVM':\n",
    "                models.append(self._hyperparameter_tuning(train_data, 'SVM'))\n",
    "            else:\n",
    "                raise ValueError('{} is not supported.'.format(algorithm))\n",
    "\n",
    "        return models\n",
    "\n",
    "    def _predict(self, models, model_features):\n",
    "        \"\"\"\n",
    "        Performs model prediction and combines the prediction made by all submodels by selecting\n",
    "        the more likely classification.\n",
    "\n",
    "        :param models: list of sklearn.models\n",
    "            Contains the trained models.\n",
    "        :param model_features: Dataframe\n",
    "            Contain the model input features.\n",
    "\n",
    "        :return:\n",
    "        np.array\n",
    "            Contains the most likely classification (duplicate or not) based on input features.\n",
    "        \"\"\"\n",
    "        predict_prob = np.zeros((len(model_features), 2))\n",
    "        for model in models:\n",
    "            predict_prob += model.predict_proba(model_features)\n",
    "        return np.argmax(predict_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}